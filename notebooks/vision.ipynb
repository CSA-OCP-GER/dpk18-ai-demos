{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Azure Cognitive Services Vision APIs\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\n\nimport requests\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.patches import Polygon\nfrom PIL import Image\nfrom io import BytesIO\nimport json\nimport time",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Computer Vision\n\nThe cloud-based Computer Vision service provides developers with access to advanced algorithms for processing images and returning information. Computer Vision works with popular image formats, such as JPEG and PNG. To analyze an image, you can either upload an image or specify an image URL. Computer Vision algorithms can analyze the content of an image in different ways, depending on the visual features you're interested in. For example, Computer Vision can determine if an image contains adult or racy content, or find all the faces in an image.\n\n**For this code to run, you need create a valid subscription key.\nIf you want to create a test key for free, have a look [here](https://azure.microsoft.com/en-us/try/cognitive-services/my-apis/).**\n\nThese examples for a majority of the features of Cognitive Services Vision, but some features such as Content Moderation, Face Identification, or Video Indexing are not shown."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vision_key = \"\" # ENTER SUBSCRIPTION KEY HERE!!!\nvision_base_url = \"https://westeurope.api.cognitive.microsoft.com/vision/v2.0/\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "analyze_url = vision_base_url + \"analyze\"\n\n# Set image_url to the URL of an image that you want to analyze.\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/\" + \\\n    \"Broadway_and_Times_Square_by_night.jpg/450px-Broadway_and_Times_Square_by_night.jpg\"\n\nheaders = {'Ocp-Apim-Subscription-Key': vision_key }\nparams  = {'visualFeatures': 'Categories,Description,Color'}\ndata    = {'url': image_url}\nresponse = requests.post(analyze_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\n\n# The 'analysis' object contains various fields that describe the image. The most\n# relevant caption for the image is obtained from the 'description' property.\nanalysis = response.json()\nprint(json.dumps(response.json(), indent=2))\nimage_caption = analysis[\"description\"][\"captions\"][0][\"text\"].capitalize()\n\n# Display the image and overlay it with the caption.\nimage = Image.open(BytesIO(requests.get(image_url).content))\nplt.imshow(image)\nplt.axis(\"off\")\n_ = plt.title(image_caption, size=\"x-large\", y=-0.1)\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "thumbnail_url = vision_base_url + \"generateThumbnail\"\n\n# Set image_url to the URL of an image that you want to analyze.\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/9/94/Bloodhound_Puppy.jpg\"\n\nheaders = {'Ocp-Apim-Subscription-Key': vision_key}\nparams  = {'width': '50', 'height': '50', 'smartCropping': 'true'}\ndata    = {'url': image_url}\nresponse = requests.post(thumbnail_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\n\nthumbnail = Image.open(BytesIO(response.content))\n\n# Display the thumbnail.\nplt.imshow(thumbnail)\nplt.axis(\"off\")\n\n# Verify the thumbnail size.\nprint(\"Thumbnail is {0}-by-{1}\".format(*thumbnail.size))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ocr_url = vision_base_url + \"ocr\"\n\n# Set image_url to the URL of an image that you want to analyze.\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/\" + \\\n    \"Atomist_quote_from_Democritus.png/338px-Atomist_quote_from_Democritus.png\"\n\nheaders = {'Ocp-Apim-Subscription-Key': vision_key}\nparams  = {'language': 'unk', 'detectOrientation': 'true'}\ndata    = {'url': image_url}\nresponse = requests.post(ocr_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\n\nanalysis = response.json()\n\n# Extract the word bounding boxes and text.\nline_infos = [region[\"lines\"] for region in analysis[\"regions\"]]\nword_infos = []\nfor line in line_infos:\n    for word_metadata in line:\n        for word_info in word_metadata[\"words\"]:\n            word_infos.append(word_info)\nword_infos\n\n# Display the image and overlay it with the extracted text.\nplt.figure(figsize=(5, 5))\nimage = Image.open(BytesIO(requests.get(image_url).content))\nax = plt.imshow(image, alpha=0.5)\nfor word in word_infos:\n    bbox = [int(num) for num in word[\"boundingBox\"].split(\",\")]\n    text = word[\"text\"]\n    origin = (bbox[0], bbox[1])\n    patch  = Rectangle(origin, bbox[2], bbox[3], fill=False, linewidth=2, color='y')\n    ax.axes.add_patch(patch)\n    plt.text(origin[0], origin[1], text, fontsize=20, weight=\"bold\", va=\"top\")\nplt.axis(\"off\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "text_recognition_url = vision_base_url + \"recognizeText\"\n\n# Set image_url to the URL of an image that you want to analyze.\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/\" + \\\n    \"Cursive_Writing_on_Notebook_paper.jpg/800px-Cursive_Writing_on_Notebook_paper.jpg\"\n\nheaders = {'Ocp-Apim-Subscription-Key': vision_key}\n# Note: The request parameter changed for APIv2.\n# For APIv1, it is 'handwriting': 'true'.\nparams  = {'mode': 'Handwritten'}\ndata    = {'url': image_url}\nresponse = requests.post(\n    text_recognition_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\n\n# Extracting handwritten text requires two API calls: One call to submit the\n# image for processing, the other to retrieve the text found in the image.\n\n# Holds the URI used to retrieve the recognized text.\noperation_url = response.headers[\"Operation-Location\"]\n\n# The recognized text isn't immediately available, so poll to wait for completion.\nanalysis = {}\npoll = True\nwhile (poll):\n    response_final = requests.get(\n        response.headers[\"Operation-Location\"], headers=headers)\n    analysis = response_final.json()\n    time.sleep(1)\n    if (\"recognitionResult\" in analysis):\n        poll= False \n    if (\"status\" in analysis and analysis['status'] == 'Failed'):\n        poll= False\n\npolygons=[]\nif (\"recognitionResult\" in analysis):\n    # Extract the recognized text, with bounding boxes.\n    polygons = [(line[\"boundingBox\"], line[\"text\"])\n        for line in analysis[\"recognitionResult\"][\"lines\"]]\n\n# Display the image and overlay it with the extracted text.\nplt.figure(figsize=(15, 15))\nimage = Image.open(BytesIO(requests.get(image_url).content))\nax = plt.imshow(image)\nfor polygon in polygons:\n    vertices = [(polygon[0][i], polygon[0][i+1])\n        for i in range(0, len(polygon[0]), 2)]\n    text     = polygon[1]\n    patch    = Polygon(vertices, closed=True, fill=False, linewidth=2, color='y')\n    ax.axes.add_patch(patch)\n    plt.text(vertices[0][0], vertices[0][1], text, fontsize=20, va=\"top\")\n_ = plt.axis(\"off\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "landmark_analyze_url = vision_base_url + \"models/landmarks/analyze\"\n\n# Set image_url to the URL of an image that you want to analyze.\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/f/f6/\" + \\\n    \"Bunker_Hill_Monument_2005.jpg\"\n\nheaders = {'Ocp-Apim-Subscription-Key': vision_key}\nparams  = {'model': 'landmarks'}\ndata    = {'url': image_url}\nresponse = requests.post(\n    landmark_analyze_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\n\n# The 'analysis' object contains various fields that describe the image. The\n# most relevant landmark for the image is obtained from the 'result' property.\nanalysis = response.json()\nassert analysis[\"result\"][\"landmarks\"] is not []\nprint(json.dumps(analysis, indent=2))\n\nlandmark_name = analysis[\"result\"][\"landmarks\"][0][\"name\"].capitalize()\n\n# Display the image and overlay it with the landmark name.\nimage = Image.open(BytesIO(requests.get(image_url).content))\nplt.imshow(image)\nplt.axis(\"off\")\n_ = plt.title(landmark_name, size=\"x-large\", y=-0.1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "celebrity_analyze_url = vision_base_url + \"models/celebrities/analyze\"\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/d/d9/Bill_gates_portrait.jpg\"\n\nheaders = {'Ocp-Apim-Subscription-Key': vision_key}\nparams  = {'model': 'celebrities'}\ndata    = {'url': image_url}\nresponse = requests.post(celebrity_analyze_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\n\nanalysis = response.json()\nassert analysis[\"result\"][\"celebrities\"] is not []\n#print(analysis)\ncelebrity_name = analysis[\"result\"][\"celebrities\"][0][\"name\"].capitalize()\n\nimage = Image.open(BytesIO(requests.get(image_url).content))\nplt.imshow(image)\nplt.axis(\"off\")\n_ = plt.title(celebrity_name, size=\"x-large\", y=-0.1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Custom Vision Service"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Image Classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install azure-cognitiveservices-vision-customvision\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nimport time\nfrom azure.cognitiveservices.vision.customvision.training import training_api\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageUrlCreateEntry\n\n# ADD VALID TRAINING AND PREDICTION KEY FROM www.customvision.ai HERE\ntraining_key = \"\"\nprediction_key = \"\"\n\ntrainer = training_api.TrainingApi(training_key)\n\n# Create a new project\nprint (\"Creating project...\")\nproject = trainer.create_project(\"Classification Project\")\n\n# Make two tags in the new project\nhemlock_tag = trainer.create_tag(project.id, \"Hemlock\")\ncherry_tag = trainer.create_tag(project.id, \"Japanese Cherry\")\n\nbase_image_url = \"https://raw.githubusercontent.com/Microsoft/Cognitive-CustomVision-Windows/master/Samples/\"\n\nprint (\"Adding images...\")\nfor image_num in range(1,10):\n    image_url = base_image_url + \"Images/Hemlock/hemlock_{}.jpg\".format(image_num)\n    trainer.create_images_from_urls(project.id, [ ImageUrlCreateEntry(url=image_url, tag_ids=[ hemlock_tag.id ] ) ])\n\nfor image_num in range(1,10):\n    image_url = base_image_url + \"Images/Japanese Cherry/japanese_cherry_{}.jpg\".format(image_num)\n    trainer.create_images_from_urls(project.id, [ ImageUrlCreateEntry(url=image_url, tag_ids=[ cherry_tag.id ] ) ])\n\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print (\"Training...\")\niteration = trainer.train_project(project.id)\nwhile (iteration.status != \"Completed\"):\n    iteration = trainer.get_iteration(project.id, iteration.id)\n    print (\"Training status: \" + iteration.status)\n    time.sleep(1)\n\n# The iteration is now trained. Make it the default project endpoint\ntrainer.update_iteration(project.id, iteration.id, is_default=True)\nprint (\"Done!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.customvision.prediction import prediction_endpoint\nfrom azure.cognitiveservices.vision.customvision.prediction.prediction_endpoint import models\n\n# Now there is a trained endpoint that can be used to make a prediction\n\npredictor = prediction_endpoint.PredictionEndpoint(prediction_key)\n\ntest_img_url = base_image_url + \"Images/Test/test_image.jpg\"\nresults = predictor.predict_image_url(project.id, iteration.id, url=test_img_url)\n\n\n# Display the image \nimage = Image.open(BytesIO(requests.get(test_img_url).content))\nplt.imshow(image)\nplt.axis(\"off\")\n\n# Display the results.\nfor prediction in results.predictions:\n    print (\"\\t\" + prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100))    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Object Detection"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\nfrom azure.cognitiveservices.vision.customvision.training import training_api\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateEntry, Region\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageUrlCreateEntry\n\n# ADD VALID TRAINING AND PREDICTION KEY FROM www.customvision.ai HERE\ntraining_key = \"\"\nprediction_key = \"\"\n\ntrainer = training_api.TrainingApi(training_key)\n\nobj_detection_domain = next(domain for domain in trainer.get_domains() if domain.type == \"ObjectDetection\")\n\n# Create a new project\nprint (\"Creating project...\")\nproject = trainer.create_project(\"DetectionProject\", domain_id=obj_detection_domain.id)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Make two tags in the new project\nfork_tag = trainer.create_tag(project.id, \"fork\")\nscissors_tag = trainer.create_tag(project.id, \"scissors\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fork_image_regions = {\n    \"fork_1\": [ 0.145833328, 0.3509314, 0.5894608, 0.238562092 ],\n    \"fork_2\": [ 0.294117659, 0.216944471, 0.534313738, 0.5980392 ],\n    \"fork_3\": [ 0.09191177, 0.0682516545, 0.757352948, 0.6143791 ],\n    \"fork_4\": [ 0.254901975, 0.185898721, 0.5232843, 0.594771266 ],\n    \"fork_5\": [ 0.2365196, 0.128709182, 0.5845588, 0.71405226 ],\n    \"fork_6\": [ 0.115196079, 0.133611143, 0.676470637, 0.6993464 ],\n    \"fork_7\": [ 0.164215669, 0.31008172, 0.767156839, 0.410130739 ],\n    \"fork_8\": [ 0.118872553, 0.318251669, 0.817401946, 0.225490168 ],\n    \"fork_9\": [ 0.18259804, 0.2136765, 0.6335784, 0.643790841 ],\n    \"fork_10\": [ 0.05269608, 0.282303959, 0.8088235, 0.452614367 ],\n    \"fork_11\": [ 0.05759804, 0.0894935, 0.9007353, 0.3251634 ],\n    \"fork_12\": [ 0.3345588, 0.07315363, 0.375, 0.9150327 ],\n    \"fork_13\": [ 0.269607842, 0.194068655, 0.4093137, 0.6732026 ],\n    \"fork_14\": [ 0.143382356, 0.218578458, 0.7977941, 0.295751631 ],\n    \"fork_15\": [ 0.19240196, 0.0633497, 0.5710784, 0.8398692 ],\n    \"fork_16\": [ 0.140931368, 0.480016381, 0.6838235, 0.240196079 ],\n    \"fork_17\": [ 0.305147052, 0.2512582, 0.4791667, 0.5408496 ],\n    \"fork_18\": [ 0.234068632, 0.445702642, 0.6127451, 0.344771236 ],\n    \"fork_19\": [ 0.219362751, 0.141781077, 0.5919118, 0.6683006 ],\n    \"fork_20\": [ 0.180147052, 0.239820287, 0.6887255, 0.235294119 ]\n}\n\nscissors_image_regions = {\n    \"scissors_1\": [ 0.4007353, 0.194068655, 0.259803921, 0.6617647 ],\n    \"scissors_2\": [ 0.426470578, 0.185898721, 0.172794119, 0.5539216 ],\n    \"scissors_3\": [ 0.289215684, 0.259428144, 0.403186262, 0.421568632 ],\n    \"scissors_4\": [ 0.343137264, 0.105833367, 0.332107842, 0.8055556 ],\n    \"scissors_5\": [ 0.3125, 0.09766343, 0.435049027, 0.71405226 ],\n    \"scissors_6\": [ 0.379901975, 0.24308826, 0.32107842, 0.5718954 ],\n    \"scissors_7\": [ 0.341911763, 0.20714055, 0.3137255, 0.6356209 ],\n    \"scissors_8\": [ 0.231617644, 0.08459154, 0.504901946, 0.8480392 ],\n    \"scissors_9\": [ 0.170343131, 0.332957536, 0.767156839, 0.403594762 ],\n    \"scissors_10\": [ 0.204656869, 0.120539248, 0.5245098, 0.743464053 ],\n    \"scissors_11\": [ 0.05514706, 0.159754932, 0.799019635, 0.730392158 ],\n    \"scissors_12\": [ 0.265931368, 0.169558853, 0.5061275, 0.606209159 ],\n    \"scissors_13\": [ 0.241421565, 0.184264734, 0.448529422, 0.6830065 ],\n    \"scissors_14\": [ 0.05759804, 0.05027781, 0.75, 0.882352948 ],\n    \"scissors_15\": [ 0.191176474, 0.169558853, 0.6936275, 0.6748366 ],\n    \"scissors_16\": [ 0.1004902, 0.279036, 0.6911765, 0.477124184 ],\n    \"scissors_17\": [ 0.2720588, 0.131977156, 0.4987745, 0.6911765 ],\n    \"scissors_18\": [ 0.180147052, 0.112369314, 0.6262255, 0.6666667 ],\n    \"scissors_19\": [ 0.333333343, 0.0274019931, 0.443627447, 0.852941155 ],\n    \"scissors_20\": [ 0.158088237, 0.04047389, 0.6691176, 0.843137264 ]\n}\n\n# Go through the data table above and create the images\nprint (\"Adding images...\")\ntagged_images_with_regions = []\n\n#images/fork/fork_1.jpg\n\n\nbase_image_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-python-sdk-samples/master/samples/vision/\"\n\n\nfor file_name in fork_image_regions.keys():\n    x,y,w,h = fork_image_regions[file_name]\n    regions = [ Region(tag_id=fork_tag.id, left=x,top=y,width=w,height=h) ]\n    image_url = base_image_url + \"images/fork/\" + file_name + \".jpg\"\n    tagged_images_with_regions.append(ImageUrlCreateEntry(url=image_url, regions=regions))\n\nfor file_name in scissors_image_regions.keys():\n    x,y,w,h = scissors_image_regions[file_name]\n    regions = [ Region(tag_id=scissors_tag.id, left=x,top=y,width=w,height=h) ]\n    image_url = base_image_url + \"images/scissors/\" + file_name + \".jpg\"\n    tagged_images_with_regions.append(ImageUrlCreateEntry(url=image_url, regions=regions))\n\ntrainer.create_images_from_urls(project.id, images=tagged_images_with_regions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print (\"Training...\")\niteration = trainer.train_project(project.id)\nwhile (iteration.status != \"Completed\"):\n    iteration = trainer.get_iteration(project.id, iteration.id)\n    print (\"Training status: \" + iteration.status)\n    time.sleep(1)\n\n# The iteration is now trained. Make it the default project endpoint\ntrainer.update_iteration(project.id, iteration.id, is_default=True)\nprint (\"Done!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.customvision.prediction import prediction_endpoint\nfrom azure.cognitiveservices.vision.customvision.prediction.prediction_endpoint import models\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.patches import Polygon\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\n\n# Now there is a trained endpoint that can be used to make a prediction\npredictor = prediction_endpoint.PredictionEndpoint(prediction_key)\n\ntest_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-python-sdk-samples/master/samples/vision/images/Test/test_od_image.jpg\"\n    \nresults = predictor.predict_image_url(project.id, url=test_url, iteration_id=iteration.id)\n\n\n# Display the results.\nfor prediction in results.predictions:\n    print (\"\\t\" + prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100), prediction.bounding_box.left, prediction.bounding_box.top, prediction.bounding_box.width, prediction.bounding_box.height)\n\n    \nplt.figure(figsize=(5, 5))\nimage = Image.open(BytesIO(requests.get(test_url).content))\nax = plt.imshow(image, alpha=0.5)\nfor prediction in results.predictions:\n    x = prediction.bounding_box.left * image.width\n    y = prediction.bounding_box.top * image.height\n    w = prediction.bounding_box.width * image.width\n    h = prediction.bounding_box.height * image.height\n    text = prediction.tag_name + \" {0:.2f}%\".format(prediction.probability * 100)\n    patch  = Rectangle((x, y), w, h, fill=False, linewidth=2, color='y')\n    ax.axes.add_patch(patch)\n    plt.text(x, y, text, fontsize=10, va=\"top\")\nplt.axis(\"off\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Face API\n\nWith Face API, we can detected face in images. Furthermore, we can identify personen, given that we have a trained there face beforehand (not shown here, but see [here](https://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/howtoidentifyfacesinimage))."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Detecting faces"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom matplotlib import patches\nfrom io import BytesIO\n\nsubscription_key = \"\" # ADD SUBSCRIPTION KEY FOR FACE API HERE\nface_api_url = 'https://westeurope.api.cognitive.microsoft.com/face/v1.0/detect'\n\n# Set image_url to the URL of an image that you want to analyze.\nimage_url = 'https://how-old.net/Images/faces2/main007.jpg'\n\nheaders = {'Ocp-Apim-Subscription-Key': subscription_key}\nparams = {\n    'returnFaceId': 'true',\n    'returnFaceLandmarks': 'false',\n    'returnFaceAttributes': 'age,gender,headPose,smile,facialHair,glasses,' +\n    'emotion,hair,makeup,occlusion,accessories,blur,exposure,noise'\n}\ndata = {'url': image_url}\nresponse = requests.post(face_api_url, params=params, headers=headers, json=data)\nfaces = response.json()\n\n# Display the original image and overlay it with the face information.\nimage = Image.open(BytesIO(requests.get(image_url).content))\nplt.figure(figsize=(8, 8))\nax = plt.imshow(image, alpha=0.6)\nfor face in faces:\n    fr = face[\"faceRectangle\"]\n    fa = face[\"faceAttributes\"]\n    origin = (fr[\"left\"], fr[\"top\"])\n    p = patches.Rectangle(\n        origin, fr[\"width\"], fr[\"height\"], fill=False, linewidth=2, color='b')\n    ax.axes.add_patch(p)\n    plt.text(origin[0], origin[1], \"%s, %d\"%(fa[\"gender\"].capitalize(), fa[\"age\"]),\n             fontsize=20, weight=\"bold\", va=\"bottom\")\n_ = plt.axis(\"off\")",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}